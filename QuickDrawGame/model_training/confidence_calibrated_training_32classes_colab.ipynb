{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ab84d0",
   "metadata": {},
   "source": [
    "# QuickDraw Model Training - 32 Classes Confidence Calibrated Edition\n",
    "## üöÄ **Google Colab Optimized for Realistic AI Confidence**\n",
    "\n",
    "**UPGRADED FROM 15 TO 32 CLASSES**: This notebook scales up the confidence-calibrated QuickDraw training to handle 32 different drawing classes instead of the original 15.\n",
    "\n",
    "## üéØ **Key Features for 32-Class Training:**\n",
    "\n",
    "### 1. **Expanded Class Support**\n",
    "- Handles 32 different QuickDraw classes simultaneously\n",
    "- Scalable architecture that maintains performance across more classes\n",
    "- Memory-optimized data loading for larger datasets\n",
    "\n",
    "### 2. **Enhanced Confidence Calibration**\n",
    "- **Label Smoothing (0.1)** - Prevents overconfidence with 32 classes\n",
    "- **Temperature Scaling** - Learnable calibration parameter\n",
    "- **Monte Carlo Dropout** - Uncertainty estimation for complex classifications\n",
    "- **Entropy Regularization** - Encourages realistic confidence across all classes\n",
    "\n",
    "### 3. **Google Colab Optimizations**\n",
    "- GPU acceleration and memory management\n",
    "- Google Drive integration for dataset access\n",
    "- Downloadable model files for local deployment\n",
    "- Progress tracking and visualization\n",
    "\n",
    "### 4. **Production-Ready Features**\n",
    "- Model saving in multiple formats (.keras, .h5, .tflite)\n",
    "- Comprehensive evaluation metrics\n",
    "- Calibration analysis across all 32 classes\n",
    "- Export capabilities for QuickDraw game integration\n",
    "\n",
    "**Expected Result**: Well-calibrated confidence scores (30-70%) across 32 classes instead of overconfident 90-100% predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9a305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies for Google Colab\n",
    "!pip install -q tensorflow==2.14.0\n",
    "!pip install -q scikit-learn\n",
    "!pip install -q opencv-python-headless\n",
    "!pip install -q matplotlib\n",
    "!pip install -q tqdm\n",
    "\n",
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# TensorFlow and Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, BatchNormalization, Layer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "print(\"üéØ QUICKDRAW 32-CLASS CONFIDENCE CALIBRATED TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ Google Colab Optimized for Realistic AI Confidence\")\n",
    "print(\"üìä Target: 30-70% confidence instead of 90-100%\")\n",
    "print(\"üé® Classes: 32 different QuickDraw categories\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"\\nüî• GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"üì± TensorFlow version: {tf.__version__}\")\n",
    "\n",
    "# Set memory growth to avoid GPU memory errors\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"‚úÖ GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"‚ö†Ô∏è GPU setup error: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed and environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d691f926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and setup paths\n",
    "from google.colab import drive, files\n",
    "import json\n",
    "\n",
    "print(\"üìÅ Setting up Google Colab environment...\")\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "\n",
    "# Configuration for 32 classes - Optimized path structure\n",
    "WORKING_DIR = \"/content/quickdraw_training\"      # Main workspace for training\n",
    "MODEL_SAVE_PATH = \"/content/models\"              # Model exports and downloads\n",
    "DATA_DIR = f\"{WORKING_DIR}/data\"                 # Preprocessed data files\n",
    "PLOTS_DIR = f\"{WORKING_DIR}/plots\"               # Training visualizations\n",
    "\n",
    "# Create working directories with clear structure\n",
    "os.makedirs(WORKING_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.chdir(WORKING_DIR)\n",
    "\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")\n",
    "print(f\"üíæ Model save path: {MODEL_SAVE_PATH}\")\n",
    "print(f\"üìä Data directory: {DATA_DIR}\")\n",
    "print(f\"üìà Plots directory: {PLOTS_DIR}\")\n",
    "\n",
    "# Training configuration - MEMORY OPTIMIZED for Google Colab\n",
    "TARGET_SIZE = 64  # Image resolution for better feature learning\n",
    "NUM_CLASSES = 32  # Upgraded from 15 to 32 classes\n",
    "MAX_SAMPLES_PER_CLASS = 3000  # üîß REDUCED: 3k per class = 96k total (was 320k)\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30  # More epochs for 32-class complexity\n",
    "USE_MIXUP = True\n",
    "\n",
    "# Memory management settings for Colab\n",
    "MEMORY_BATCH_SIZE = 5000  # Process data in chunks of 5k samples\n",
    "ENABLE_MEMORY_OPTIMIZATION = True  # Enable memory-efficient loading\n",
    "\n",
    "# Updated 32 QuickDraw classes (matching your modified data loading script)\n",
    "QUICKDRAW_32_CLASSES = [\n",
    "    'airplane', 'apple', ' banana', 'bicycle', 'bowtie', 'bus', 'candle', \n",
    "    'car', 'cat', 'computer', 'dog', 'door', 'elephant', 'envelope', 'fish', 'flower', 'guitar', \n",
    "    'horse', 'house', 'ice cream', 'lightning', 'moon', 'mountain', 'rabbit', 'smiley face',\n",
    "    'star', 'sun', 'tent', 'toothbrush', 'tree', 'truck', 'wristwatch'\n",
    "]\n",
    "\n",
    "print(f\"\\nüîß Training Configuration:\")\n",
    "print(f\"   ‚Ä¢ Target image size: {TARGET_SIZE}x{TARGET_SIZE}\")\n",
    "print(f\"   ‚Ä¢ Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"   ‚Ä¢ Max samples per class: {MAX_SAMPLES_PER_CLASS:,}\")\n",
    "print(f\"   ‚Ä¢ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   ‚Ä¢ Epochs: {EPOCHS}\")\n",
    "print(f\"   ‚Ä¢ Mixup augmentation: {USE_MIXUP}\")\n",
    "print(f\"   ‚Ä¢ Data loading: Consistent with original approach\")\n",
    "\n",
    "# Display class list\n",
    "print(f\"\\nüé® Target Classes ({len(QUICKDRAW_32_CLASSES)}):\")\n",
    "for i, class_name in enumerate(QUICKDRAW_32_CLASSES[:8]):\n",
    "    print(f\"   {i+1:2d}. {class_name}\")\n",
    "print(f\"   ... and {len(QUICKDRAW_32_CLASSES)-8} more classes\")\n",
    "\n",
    "print(f\"\\nüí° Optimized File Structure:\")\n",
    "print(f\"   üìã Data files: {DATA_DIR}/features_32classes, {DATA_DIR}/labels_32classes\")\n",
    "print(f\"   üìà Plots/visualizations: {PLOTS_DIR}/\")\n",
    "print(f\"   üíæ Trained models: {MODEL_SAVE_PATH}/\")\n",
    "print(f\"   üîÑ Same preprocessing pipeline as original 15-class model\")\n",
    "print(f\"   ‚úÖ Maintains consistency with confidence_calibrated_training.ipynb\")\n",
    "\n",
    "print(f\"\\n‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading utilities (simplified for consistency)\n",
    "\n",
    "def create_data_visualization(labels, num_classes, class_names):\n",
    "    \"\"\"\n",
    "    Visualize class distribution for 32 classes with updated class names\n",
    "    Saves plots to organized PLOTS_DIR\n",
    "    \"\"\"\n",
    "    print(f\"\\nüìä Creating class distribution visualization...\")\n",
    "    \n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    \n",
    "    # Create subplot for better visualization of 32 classes\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "    \n",
    "    # Bar plot\n",
    "    ax1.bar(range(num_classes), counts, alpha=0.7, color='skyblue', edgecolor='navy')\n",
    "    ax1.set_title(f'QuickDraw Dataset - Class Distribution ({num_classes} Classes)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Class ID', fontsize=12)\n",
    "    ax1.set_ylabel('Number of Samples', fontsize=12)\n",
    "    ax1.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars (every 4th bar to avoid clutter)\n",
    "    for i, count in enumerate(counts):\n",
    "        if i % 4 == 0:\n",
    "            ax1.text(i, count + 50, f'{count:,}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    # Pie chart showing distribution\n",
    "    ax2.pie(counts, labels=[f'C{i}' for i in range(num_classes)], \n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    ax2.set_title('Class Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to organized plots directory\n",
    "    plot_path = f'{PLOTS_DIR}/class_distribution_32classes.png'\n",
    "    plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"‚úÖ Distribution plot saved: {plot_path}\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"üìà Distribution Statistics:\")\n",
    "    print(f\"   Mean samples per class: {np.mean(counts):.0f}\")\n",
    "    print(f\"   Min samples: {np.min(counts):,}\")\n",
    "    print(f\"   Max samples: {np.max(counts):,}\")\n",
    "    print(f\"   Standard deviation: {np.std(counts):.0f}\")\n",
    "    \n",
    "    # Display class names mapping\n",
    "    print(f\"\\nüé® Class Names Mapping:\")\n",
    "    for i, class_name in enumerate(class_names[:min(16, len(class_names))]):\n",
    "        print(f\"   {i:2d}: {class_name}\")\n",
    "    if len(class_names) > 16:\n",
    "        print(f\"   ... and {len(class_names)-16} more classes\")\n",
    "\n",
    "print(\"‚úÖ Data utilities defined!\")\n",
    "print(\"üìã Note: Using simplified approach consistent with original training notebook\")\n",
    "print(f\"üí° Expects preprocessed pickle files in: {DATA_DIR}/\")\n",
    "print(f\"üìà Saves visualizations to: {PLOTS_DIR}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f894fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess 32-class dataset (consistent with original approach)\n",
    "\n",
    "def load_and_preprocess_data_32class_memory_efficient(target_size=64):\n",
    "    \"\"\"\n",
    "    üß† MEMORY-EFFICIENT: Load and preprocess QuickDraw data for 32 classes \n",
    "    Processes data in chunks to avoid Google Colab RAM limits\n",
    "    \"\"\"\n",
    "    import psutil\n",
    "    import os\n",
    "    import gc  # Move gc import to the top\n",
    "    \n",
    "    # Check available memory\n",
    "    available_memory = psutil.virtual_memory().available / (1024**3)  # GB\n",
    "    print(f\"üß† Available RAM: {available_memory:.1f} GB\")\n",
    "    \n",
    "    # Load data using explicit paths relative to DATA_DIR\n",
    "    features_path = f\"{DATA_DIR}/features_32classes\"\n",
    "    labels_path = f\"{DATA_DIR}/labels_32classes\"\n",
    "    \n",
    "    try:\n",
    "        # üîß MEMORY OPTIMIZATION: Load in chunks to check size first\n",
    "        print(f\"üì• Checking data file sizes...\")\n",
    "        \n",
    "        with open(features_path, \"rb\") as f:\n",
    "            # Read just the header to get size info\n",
    "            temp_features = pickle.load(f)\n",
    "            total_samples = len(temp_features) if hasattr(temp_features, '__len__') else temp_features.shape[0]\n",
    "            \n",
    "        with open(labels_path, \"rb\") as f:\n",
    "            temp_labels = pickle.load(f)\n",
    "            \n",
    "        print(f\"üìä Total samples in dataset: {total_samples:,}\")\n",
    "        \n",
    "        # Calculate memory usage estimate\n",
    "        estimated_memory = (total_samples * target_size * target_size * 4) / (1024**3)  # GB\n",
    "        print(f\"üíæ Estimated memory needed: {estimated_memory:.1f} GB\")\n",
    "        \n",
    "        # Apply intelligent sampling if needed\n",
    "        if estimated_memory > available_memory * 0.8:  # Use 80% of available memory\n",
    "            max_safe_samples = int((available_memory * 0.8 * 1024**3) / (target_size * target_size * 4))\n",
    "            samples_per_class = min(MAX_SAMPLES_PER_CLASS, max_safe_samples // NUM_CLASSES)\n",
    "            total_safe_samples = samples_per_class * NUM_CLASSES\n",
    "            \n",
    "            print(f\"‚ö†Ô∏è  Dataset too large for available memory!\")\n",
    "            print(f\"üîß Auto-reducing to {samples_per_class:,} samples per class\")\n",
    "            print(f\"üìâ Total samples: {total_safe_samples:,} (was {total_samples:,})\")\n",
    "            \n",
    "            # Intelligent sampling - take evenly distributed samples\n",
    "            if hasattr(temp_features, '__len__'):\n",
    "                features = temp_features\n",
    "                labels = temp_labels\n",
    "            else:\n",
    "                features = temp_features\n",
    "                labels = temp_labels\n",
    "                \n",
    "            # Sample evenly across classes\n",
    "            sampled_features = []\n",
    "            sampled_labels = []\n",
    "            \n",
    "            for class_id in range(NUM_CLASSES):\n",
    "                class_mask = np.array(labels) == class_id\n",
    "                class_indices = np.where(class_mask)[0]\n",
    "                \n",
    "                if len(class_indices) > samples_per_class:\n",
    "                    # Take evenly spaced samples\n",
    "                    step = len(class_indices) // samples_per_class\n",
    "                    selected_indices = class_indices[::step][:samples_per_class]\n",
    "                else:\n",
    "                    selected_indices = class_indices\n",
    "                    \n",
    "                for idx in selected_indices:\n",
    "                    sampled_features.append(features[idx])\n",
    "                    sampled_labels.append(labels[idx])\n",
    "                    \n",
    "                print(f\"   Class {class_id} ({QUICKDRAW_32_CLASSES[class_id] if class_id < len(QUICKDRAW_32_CLASSES) else f'Class_{class_id}'}): {len(selected_indices):,} samples\")\n",
    "            \n",
    "            features = np.array(sampled_features)\n",
    "            labels = np.array(sampled_labels)\n",
    "            \n",
    "            # Clean up temporary variables\n",
    "            del temp_features, temp_labels, sampled_features, sampled_labels\n",
    "            gc.collect()\n",
    "            \n",
    "        else:\n",
    "            # Load full dataset if memory allows\n",
    "            features = temp_features\n",
    "            labels = temp_labels\n",
    "            print(f\"‚úÖ Loading full dataset - memory sufficient\")\n",
    "            \n",
    "        print(f\"üì• Final data loaded: {features.shape}, labels: {len(labels)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Preprocessed 32-class data files not found!\")\n",
    "        print(\"üí° Please run load_data_32classes_colab.py first to generate the data files\")\n",
    "        print(f\"üìã Expected files: {features_path}, {labels_path}\")\n",
    "        print(f\"üìÇ Expected location: {DATA_DIR}/\")\n",
    "        \n",
    "        # CREATE OPTIMIZED DUMMY DATA FOR TESTING\n",
    "        print(f\"\\nüîß Creating memory-optimized dummy data for testing...\")\n",
    "        samples_per_class = 500  # Much smaller for testing\n",
    "        total_samples = samples_per_class * NUM_CLASSES\n",
    "        features = np.random.rand(total_samples, 28, 28).astype('float32') * 255\n",
    "        labels = np.repeat(range(NUM_CLASSES), samples_per_class)\n",
    "        print(f\"‚ö†Ô∏è  Using dummy data: {features.shape}, {len(labels)} labels\")\n",
    "        print(f\"üö® REPLACE WITH REAL DATA BEFORE PRODUCTION USE!\")\n",
    "    \n",
    "    # üîß MEMORY-EFFICIENT UPSCALING: Process in batches\n",
    "    if target_size != 28:\n",
    "        print(f\"üîÑ Memory-efficient upscaling: 28x28 ‚Üí {target_size}x{target_size}...\")\n",
    "        \n",
    "        # Process in batches to save memory\n",
    "        batch_size = min(MEMORY_BATCH_SIZE, len(features))\n",
    "        features_resized = np.zeros((features.shape[0], target_size, target_size), dtype='float32')\n",
    "        \n",
    "        for start_idx in range(0, len(features), batch_size):\n",
    "            end_idx = min(start_idx + batch_size, len(features))\n",
    "            batch_size_actual = end_idx - start_idx\n",
    "            \n",
    "            print(f\"   Processing batch {start_idx//batch_size + 1}/{(len(features)-1)//batch_size + 1}: samples {start_idx}-{end_idx-1}\")\n",
    "            \n",
    "            # Process batch\n",
    "            for i in range(start_idx, end_idx):\n",
    "                relative_i = i - start_idx\n",
    "                img_2d = features[i].reshape(28, 28) if features[i].ndim == 1 else features[i]\n",
    "                features_resized[i] = cv2.resize(img_2d, (target_size, target_size), interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "            # Force garbage collection every batch\n",
    "            if start_idx > 0:\n",
    "                gc.collect()\n",
    "        \n",
    "        features = features_resized\n",
    "        print(f\"‚úÖ Memory-efficient upscaling complete: {features.shape}\")\n",
    "    \n",
    "    # Shuffle data (consistent with original)\n",
    "    features, labels = shuffle(features, labels, random_state=42)\n",
    "    \n",
    "    # Convert labels to categorical for 32 classes\n",
    "    labels_categorical = tf.keras.utils.to_categorical(labels, num_classes=NUM_CLASSES)\n",
    "    \n",
    "    # Split: 70% train, 15% validation, 15% test (same as original)\n",
    "    train_x, temp_x, train_y, temp_y = train_test_split(\n",
    "        features, labels_categorical, test_size=0.3, random_state=42, stratify=labels_categorical\n",
    "    )\n",
    "    val_x, test_x, val_y, test_y = train_test_split(\n",
    "        temp_x, temp_y, test_size=0.5, random_state=42, stratify=temp_y\n",
    "    )\n",
    "    \n",
    "    # Reshape for CNN (same as original)\n",
    "    train_x = train_x.reshape(-1, target_size, target_size, 1)\n",
    "    val_x = val_x.reshape(-1, target_size, target_size, 1)\n",
    "    test_x = test_x.reshape(-1, target_size, target_size, 1)\n",
    "    \n",
    "    # Normalize to [0, 1] (consistent approach)\n",
    "    train_x = train_x.astype('float32') / 255.0\n",
    "    val_x = val_x.astype('float32') / 255.0\n",
    "    test_x = test_x.astype('float32') / 255.0\n",
    "    \n",
    "    print(f\"üìä Final data split: Train={len(train_x):,}, Val={len(val_x):,}, Test={len(test_x):,}\")\n",
    "    \n",
    "    # Final memory cleanup\n",
    "    del features, labels, labels_categorical, temp_x, temp_y\n",
    "    gc.collect()\n",
    "    \n",
    "    # Check final memory usage\n",
    "    final_memory = psutil.virtual_memory().available / (1024**3)\n",
    "    print(f\"üß† Remaining RAM: {final_memory:.1f} GB\")\n",
    "    \n",
    "    return train_x, val_x, test_x, train_y, val_y, test_y\n",
    "\n",
    "print(\"üöÄ Loading and preprocessing 32-class QuickDraw dataset...\")\n",
    "print(\"üìã Using consistent approach from original confidence calibrated training\")\n",
    "print(f\"üìÇ Looking for data files in: {DATA_DIR}/\")\n",
    "\n",
    "# Load and preprocess data using MEMORY-EFFICIENT approach\n",
    "train_x, val_x, test_x, train_y, val_y, test_y = load_and_preprocess_data_32class_memory_efficient(TARGET_SIZE)\n",
    "\n",
    "# Check if data loading was successful\n",
    "if train_x is not None and len(train_x) > 0:\n",
    "    print(f\"\\n‚úÖ Data preprocessing complete!\")\n",
    "    print(f\"üìä Dataset Split Summary:\")\n",
    "    print(f\"   üéØ Training set: {len(train_x):,} samples\")\n",
    "    print(f\"   üî¨ Validation set: {len(val_x):,} samples\")\n",
    "    print(f\"   üß™ Test set: {len(test_x):,} samples\")\n",
    "    print(f\"   üìè Input shape: {train_x.shape[1:]}\")\n",
    "    print(f\"   üé® Number of classes: {NUM_CLASSES}\")\n",
    "    \n",
    "    # Create class distribution visualization\n",
    "    original_labels = np.argmax(train_y, axis=1)  # Convert back to get original labels for visualization\n",
    "    create_data_visualization(original_labels, NUM_CLASSES, QUICKDRAW_32_CLASSES)\n",
    "    \n",
    "    # Display sample images with class names\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "    for i in range(16):\n",
    "        row, col = i // 8, i % 8\n",
    "        class_idx = np.argmax(train_y[i])\n",
    "        class_name = QUICKDRAW_32_CLASSES[class_idx] if class_idx < len(QUICKDRAW_32_CLASSES) else f\"Class_{class_idx}\"\n",
    "        \n",
    "        axes[row, col].imshow(train_x[i].squeeze(), cmap='gray')\n",
    "        axes[row, col].set_title(f'{class_name}', fontsize=9)\n",
    "        axes[row, col].axis('off')\n",
    "    plt.suptitle('Sample Training Images (32-Class Dataset)', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save to plots directory\n",
    "    sample_plot_path = f'{PLOTS_DIR}/sample_images_32classes.png'\n",
    "    plt.savefig(sample_plot_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"üìà Sample images saved: {sample_plot_path}\")\n",
    "    \n",
    "    print(f\"üéâ Ready for confidence-calibrated training with {NUM_CLASSES} classes!\")\n",
    "    print(f\"üìã Class names updated to match your data loading script\")\n",
    "else:\n",
    "    print(\"‚ùå Data loading failed completely.\")\n",
    "    print(\"üîß Please check your data files and paths\")\n",
    "    # Set variables to None to prevent downstream errors\n",
    "    train_x = val_x = test_x = train_y = val_y = test_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee8e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence calibration components for 32-class model\n",
    "\n",
    "class TemperatureScaling(Layer):\n",
    "    \"\"\"\n",
    "    Learnable temperature scaling layer for confidence calibration\n",
    "    Optimized for 32-class classification\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TemperatureScaling, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Learnable temperature parameter (initialized to 1.0)\n",
    "        self.temperature = self.add_weight(\n",
    "            name='temperature',\n",
    "            shape=(),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "            constraint=tf.keras.constraints.NonNeg()  # Ensure positive\n",
    "        )\n",
    "        super(TemperatureScaling, self).build(input_shape)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Apply temperature scaling: logits / temperature\n",
    "        return inputs / (self.temperature + 1e-8)\n",
    "\n",
    "class ConfidenceRegularizer(tf.keras.regularizers.Regularizer):\n",
    "    \"\"\"\n",
    "    Custom regularizer that penalizes overconfident predictions\n",
    "    Adapted for 32-class scenarios\n",
    "    \"\"\"\n",
    "    def __init__(self, strength=0.05):  # Reduced strength for 32 classes\n",
    "        self.strength = strength\n",
    "    \n",
    "    def __call__(self, predictions):\n",
    "        # Calculate entropy (higher entropy = less confident = good)\n",
    "        entropy = -tf.reduce_sum(predictions * tf.math.log(predictions + 1e-10), axis=-1)\n",
    "        # Penalize low entropy (high confidence)\n",
    "        max_entropy = tf.math.log(tf.cast(tf.shape(predictions)[-1], tf.float32))\n",
    "        confidence_penalty = self.strength * tf.reduce_mean(max_entropy - entropy)\n",
    "        return confidence_penalty\n",
    "\n",
    "class CalibrationCallback(Callback):\n",
    "    \"\"\"\n",
    "    Monitor calibration during training for 32-class model\n",
    "    \"\"\"\n",
    "    def __init__(self, validation_data):\n",
    "        self.validation_data = validation_data\n",
    "        self.confidence_history = []\n",
    "        self.accuracy_history = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_x, val_y = self.validation_data\n",
    "        predictions = self.model.predict(val_x, verbose=0)\n",
    "        \n",
    "        # Calculate average confidence\n",
    "        max_confidences = np.max(predictions, axis=1)\n",
    "        avg_confidence = np.mean(max_confidences)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predicted_classes = np.argmax(predictions, axis=1)\n",
    "        true_classes = np.argmax(val_y, axis=1)\n",
    "        accuracy = np.mean(predicted_classes == true_classes)\n",
    "        \n",
    "        # Store history\n",
    "        self.confidence_history.append(avg_confidence)\n",
    "        self.accuracy_history.append(accuracy)\n",
    "        \n",
    "        # Calculate calibration error\n",
    "        calibration_error = abs(avg_confidence - accuracy)\n",
    "        \n",
    "        # Check for overconfidence in different ranges\n",
    "        very_high_conf = np.sum(max_confidences > 0.95) / len(max_confidences) * 100\n",
    "        high_conf = np.sum(max_confidences > 0.8) / len(max_confidences) * 100\n",
    "        moderate_conf = np.sum((max_confidences >= 0.5) & (max_confidences <= 0.8)) / len(max_confidences) * 100\n",
    "        \n",
    "        print(f\"\\nüìä Calibration Metrics - Epoch {epoch + 1}:\")\n",
    "        print(f\"   Average Confidence: {avg_confidence:.3f} ({avg_confidence*100:.1f}%)\")\n",
    "        print(f\"   Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "        print(f\"   Calibration Error: {calibration_error:.3f}\")\n",
    "        print(f\"   Confidence Distribution:\")\n",
    "        print(f\"     >95%: {very_high_conf:.1f}% | 80-95%: {high_conf-very_high_conf:.1f}% | 50-80%: {moderate_conf:.1f}%\")\n",
    "        \n",
    "        if avg_confidence > 0.85:\n",
    "            print(f\"   üö® HIGH CONFIDENCE WARNING - Model may be overconfident!\")\n",
    "        elif avg_confidence > 0.75:\n",
    "            print(f\"   ‚ö†Ô∏è  Moderate confidence - monitor calibration\")\n",
    "        else:\n",
    "            print(f\"   ‚úÖ Good confidence level for 32-class problem\")\n",
    "\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"\n",
    "    Mixup data augmentation for better calibration with 32 classes\n",
    "    \"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.shape[0]\n",
    "    index = np.random.permutation(batch_size)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    \n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def create_calibrated_model_32class(image_size=64, num_classes=32, use_temperature=True):\n",
    "    \"\"\"\n",
    "    Create a confidence-calibrated QuickDraw model for 32 classes\n",
    "    \n",
    "    Key Features:\n",
    "    - Optimized architecture for 32-class classification\n",
    "    - Enhanced regularization for larger class space\n",
    "    - Temperature scaling for confidence calibration\n",
    "    - Monte Carlo Dropout for uncertainty estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Enhanced architecture for 32 classes\n",
    "    # First conv block\n",
    "    model.add(Conv2D(64, (5, 5), input_shape=(image_size, image_size, 1), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "    \n",
    "    # Second conv block\n",
    "    model.add(Conv2D(128, (5, 5), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "    \n",
    "    # Third conv block (essential for 64x64 and 32 classes)\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "    \n",
    "    # Fourth conv block for better feature extraction\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "    \n",
    "    # Dense layers with enhanced dropout for 32 classes\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Higher dropout for 32 classes\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    \n",
    "    # Output layer (logits)\n",
    "    model.add(Dense(num_classes))\n",
    "    \n",
    "    # Add temperature scaling layer\n",
    "    if use_temperature:\n",
    "        model.add(TemperatureScaling())\n",
    "    \n",
    "    # Final softmax activation\n",
    "    model.add(tf.keras.layers.Activation('softmax'))\n",
    "    \n",
    "    # Compile with label smoothing optimized for 32 classes\n",
    "    # Fixed: Use correct metric name for TensorFlow 2.14+\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "            label_smoothing=0.1,  # Effective for larger class spaces\n",
    "            from_logits=False\n",
    "        ),\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            learning_rate=0.001,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999\n",
    "        ),\n",
    "        metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_accuracy')]\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Calibrated 32-class model created:\")\n",
    "    print(f\"   ‚Ä¢ Architecture: Enhanced CNN for 32 classes\")\n",
    "    print(f\"   ‚Ä¢ Label smoothing: 0.1 (prevents overconfidence)\")\n",
    "    print(f\"   ‚Ä¢ Temperature scaling: {use_temperature}\")\n",
    "    print(f\"   ‚Ä¢ Monte Carlo Dropout: Enhanced for complexity\")\n",
    "    print(f\"   ‚Ä¢ Input shape: ({image_size}, {image_size}, 1)\")\n",
    "    print(f\"   ‚Ä¢ Output classes: {num_classes}\")\n",
    "    print(f\"   ‚Ä¢ Metrics: accuracy, top_5_accuracy (fixed for TF 2.14+)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Confidence calibration components defined for 32-class model!\")\n",
    "print(\"üîß Fixed: Updated top_5_accuracy metric for TensorFlow 2.14+ compatibility\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ccb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training configuration and callbacks for 32-class model\n",
    "\n",
    "print(\"üîß Setting up training configuration for 32-class model...\")\n",
    "\n",
    "# Verify data is available before proceeding\n",
    "if train_x is None or len(train_x) == 0:\n",
    "    print(\"‚ùå Cannot proceed with training - no data loaded!\")\n",
    "    print(\"üí° Please ensure data files are available or re-run the data loading cell\")\n",
    "    print(\"üîß Expected files: features_32classes, labels_32classes in DATA_DIR\")\n",
    "else:\n",
    "    print(f\"‚úÖ Data verified: {len(train_x):,} training samples ready\")\n",
    "\n",
    "    # Create the calibrated model\n",
    "    model = create_calibrated_model_32class(\n",
    "        image_size=TARGET_SIZE, \n",
    "        num_classes=NUM_CLASSES, \n",
    "        use_temperature=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\nüìã Model Architecture Summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    # Calculate model parameters\n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([tf.keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "    print(f\"\\nüìä Model Statistics:\")\n",
    "    print(f\"   Total parameters: {total_params:,}\")\n",
    "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   Model size estimate: ~{total_params * 4 / 1024**2:.1f} MB\")\n",
    "\n",
    "    # Enhanced callbacks for 32-class training\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            f'{MODEL_SAVE_PATH}/QuickDraw_CALIBRATED_32class_{TARGET_SIZE}x{TARGET_SIZE}.keras',\n",
    "            monitor='val_accuracy',\n",
    "            verbose=1,\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            save_weights_only=False\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,  # Increased patience for 32 classes\n",
    "            restore_best_weights=True,\n",
    "            verbose=1,\n",
    "            min_delta=0.001\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,  # Adjusted for 32-class complexity\n",
    "            min_lr=1e-7,\n",
    "            verbose=1,\n",
    "            cooldown=2\n",
    "        ),\n",
    "        CalibrationCallback(validation_data=(val_x, val_y))\n",
    "    ]\n",
    "\n",
    "    # Enhanced data augmentation for 32-class diversity\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=15,        # Slightly more rotation\n",
    "        width_shift_range=0.1,    # More shift for variety\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,           # More zoom variation\n",
    "        shear_range=0.08,         # More shear for robustness\n",
    "        fill_mode='constant',\n",
    "        cval=0,\n",
    "        horizontal_flip=False,    # No flip for drawings\n",
    "        vertical_flip=False\n",
    "    )\n",
    "\n",
    "    # Additional augmentation for difficult classes\n",
    "    def custom_augmentation_batch(x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Apply additional augmentation for better generalization\n",
    "        \"\"\"\n",
    "        # Apply mixup with probability\n",
    "        if USE_MIXUP and np.random.random() > 0.7:\n",
    "            x_batch, y_a, y_b, lam = mixup_data(x_batch, y_batch, alpha=0.2)\n",
    "            return x_batch, (y_a, y_b, lam)\n",
    "        return x_batch, y_batch\n",
    "\n",
    "    print(f\"\\n‚úÖ Training configuration complete:\")\n",
    "    print(f\"   üéØ Model: 32-class confidence-calibrated CNN\")\n",
    "    print(f\"   üìä Callbacks: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CalibrationCallback\")\n",
    "    print(f\"   üîÑ Data augmentation: Enhanced for 32-class diversity\")\n",
    "    print(f\"   üì¶ Mixup: {USE_MIXUP} (probability: 30%)\")\n",
    "    print(f\"   ‚è±Ô∏è  Training epochs: {EPOCHS}\")\n",
    "    print(f\"   üìà Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "    # Fit data generator only if we have valid training data\n",
    "    print(f\"\\nüîÑ Fitting data generator...\")\n",
    "    try:\n",
    "        datagen.fit(train_x)\n",
    "        print(f\"‚úÖ Data generator fitted successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Data generator fitting failed: {e}\")\n",
    "        print(f\"üîß Training data shape: {train_x.shape if train_x is not None else 'None'}\")\n",
    "        raise\n",
    "\n",
    "    print(f\"‚úÖ Ready to start confidence-calibrated training!\")\n",
    "    print(f\"üéØ Expected: Well-calibrated confidence scores for 32 classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b36887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the 32-class confidence-calibrated model\n",
    "\n",
    "# Check if training prerequisites are met\n",
    "if 'model' not in locals() or model is None:\n",
    "    print(\"‚ùå Model not defined! Please run the previous cells first.\")\n",
    "    print(\"üîß Make sure to run the training configuration cell\")\n",
    "else:\n",
    "    print(\"üöÄ Starting CONFIDENCE CALIBRATED training for 32 classes...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üéØ Goal: Achieve realistic confidence (30-70%) across 32 classes\")\n",
    "    print(\"üîß Techniques: Label smoothing + Temperature scaling + Enhanced regularization\")\n",
    "    print(\"üìä Architecture: Deep CNN optimized for 32-class complexity\")\n",
    "\n",
    "    # Start training with progress tracking\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Training with enhanced monitoring\n",
    "    print(f\"\\nüèÉ‚Äç‚ôÇÔ∏è Training started at: {time.strftime('%H:%M:%S')}\")\n",
    "    print(f\"üìä Training samples: {len(train_x):,}\")\n",
    "    print(f\"üî¨ Validation samples: {len(val_x):,}\")\n",
    "    print(f\"‚öôÔ∏è  Steps per epoch: {len(train_x) // BATCH_SIZE}\")\n",
    "\n",
    "    # Fix for TensorFlow 2.14+ compatibility - use direct data instead of generator\n",
    "    print(f\"\\nüîß Using direct data approach for TensorFlow 2.14+ compatibility...\")\n",
    "    \n",
    "    # Apply data augmentation to training data (one-time augmentation)\n",
    "    print(\"üîÑ Applying data augmentation...\")\n",
    "    augmented_x = []\n",
    "    augmented_y = []\n",
    "    \n",
    "    # Create multiple augmented versions of the training data\n",
    "    num_augmentations = 3  # Create 3 augmented versions\n",
    "    \n",
    "    for aug_idx in range(num_augmentations):\n",
    "        print(f\"   Creating augmentation set {aug_idx + 1}/{num_augmentations}...\")\n",
    "        \n",
    "        # Apply augmentation using the data generator\n",
    "        batch_size_aug = min(100, len(train_x))  # Process in batches\n",
    "        for i in range(0, len(train_x), batch_size_aug):\n",
    "            end_idx = min(i + batch_size_aug, len(train_x))\n",
    "            batch_x = train_x[i:end_idx]\n",
    "            batch_y = train_y[i:end_idx]\n",
    "            \n",
    "            # Apply augmentation\n",
    "            aug_generator = datagen.flow(batch_x, batch_y, batch_size=len(batch_x), shuffle=False)\n",
    "            aug_batch_x, aug_batch_y = next(aug_generator)\n",
    "            \n",
    "            augmented_x.append(aug_batch_x)\n",
    "            augmented_y.append(aug_batch_y)\n",
    "    \n",
    "    # Add original data\n",
    "    augmented_x.append(train_x)\n",
    "    augmented_y.append(train_y)\n",
    "    \n",
    "    # Combine all augmented data\n",
    "    final_train_x = np.concatenate(augmented_x, axis=0)\n",
    "    final_train_y = np.concatenate(augmented_y, axis=0)\n",
    "    \n",
    "    # Shuffle combined data\n",
    "    from sklearn.utils import shuffle as sklearn_shuffle\n",
    "    final_train_x, final_train_y = sklearn_shuffle(final_train_x, final_train_y, random_state=42)\n",
    "    \n",
    "    print(f\"‚úÖ Augmented dataset ready: {len(final_train_x):,} samples (original: {len(train_x):,})\")\n",
    "    \n",
    "    # Execute training with direct data (fixed approach)\n",
    "    history = model.fit(\n",
    "        final_train_x, final_train_y,\n",
    "        validation_data=(val_x, val_y),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "        shuffle=True  # Enable shuffling for better training\n",
    "    )\n",
    "\n",
    "    # Training completion\n",
    "    end_time = time.time()\n",
    "    training_duration = end_time - start_time\n",
    "\n",
    "    print(f\"\\n‚úÖ Training completed!\")\n",
    "    print(f\"‚è±Ô∏è  Total training time: {training_duration//3600:.0f}h {(training_duration%3600)//60:.0f}m {training_duration%60:.0f}s\")\n",
    "\n",
    "    # Plot training history\n",
    "    print(f\"\\nüìà Plotting training history...\")\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Accuracy plots\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "    ax1.set_title('Model Accuracy (32 Classes)', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Loss plots\n",
    "    ax2.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Top-5 accuracy (relevant for 32 classes)\n",
    "    if 'top_5_accuracy' in history.history:\n",
    "        ax3.plot(history.history['top_5_accuracy'], label='Training Top-5 Accuracy', linewidth=2)\n",
    "        ax3.plot(history.history['val_top_5_accuracy'], label='Validation Top-5 Accuracy', linewidth=2)\n",
    "        ax3.set_title('Top-5 Accuracy (32 Classes)', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Epoch')\n",
    "        ax3.set_ylabel('Top-5 Accuracy')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        # Plot learning rate if available\n",
    "        if 'lr' in history.history:\n",
    "            ax3.plot(history.history['lr'], label='Learning Rate', linewidth=2, color='green')\n",
    "            ax3.set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "            ax3.set_xlabel('Epoch')\n",
    "            ax3.set_ylabel('Learning Rate')\n",
    "            ax3.set_yscale('log')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # Confidence calibration trend (if available from callback)\n",
    "    if len(callbacks) > 3 and hasattr(callbacks[3], 'confidence_history'):\n",
    "        ax4.plot(callbacks[3].confidence_history, label='Avg Confidence', linewidth=2, color='orange')\n",
    "        ax4.plot(callbacks[3].accuracy_history, label='Accuracy', linewidth=2, color='blue')\n",
    "        ax4.set_title('Confidence vs Accuracy Trend', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Score')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add ideal calibration line\n",
    "        ax4.axline((0, 0), slope=1, color='red', linestyle='--', alpha=0.5, label='Perfect Calibration')\n",
    "    else:\n",
    "        # Alternative plot - training progress\n",
    "        epochs_range = range(1, len(history.history['accuracy']) + 1)\n",
    "        ax4.plot(epochs_range, history.history['accuracy'], label='Training Progress', linewidth=2, color='purple')\n",
    "        ax4.set_title('Training Progress Overview', fontsize=14, fontweight='bold')\n",
    "        ax4.set_xlabel('Epoch')\n",
    "        ax4.set_ylabel('Training Accuracy')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save plot to organized plots directory\n",
    "    plot_save_path = f'{PLOTS_DIR}/training_history_32classes.png'\n",
    "    plt.savefig(plot_save_path, dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"üìà Training history saved: {plot_save_path}\")\n",
    "\n",
    "    # Display final training metrics\n",
    "    final_train_acc = max(history.history['accuracy'])\n",
    "    final_val_acc = max(history.history['val_accuracy'])\n",
    "    final_train_loss = min(history.history['loss'])\n",
    "    final_val_loss = min(history.history['val_loss'])\n",
    "\n",
    "    print(f\"\\nüìä Final Training Metrics:\")\n",
    "    print(f\"   üéØ Best Training Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
    "    print(f\"   ‚úÖ Best Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
    "    print(f\"   üìâ Final Training Loss: {final_train_loss:.4f}\")\n",
    "    print(f\"   üìâ Final Validation Loss: {final_val_loss:.4f}\")\n",
    "\n",
    "    if 'top_5_accuracy' in history.history:\n",
    "        final_top5_acc = max(history.history['val_top_5_accuracy'])\n",
    "        print(f\"   üèÜ Best Top-5 Accuracy: {final_top5_acc:.4f} ({final_top5_acc*100:.2f}%)\")\n",
    "\n",
    "    # Calculate improvement from augmentation\n",
    "    augmentation_factor = len(final_train_x) / len(train_x)\n",
    "    print(f\"   üìà Data augmentation factor: {augmentation_factor:.1f}x\")\n",
    "\n",
    "    # Check for potential issues\n",
    "    if final_val_acc < 0.3:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Low validation accuracy detected!\")\n",
    "        print(f\"üí° This might be due to dummy data. Use real QuickDraw data for better results.\")\n",
    "    elif final_val_acc > 0.95:\n",
    "        print(f\"\\nüö® Very high validation accuracy - check for overfitting!\")\n",
    "    else:\n",
    "        print(f\"\\n‚úÖ Training metrics look reasonable for 32-class classification!\")\n",
    "\n",
    "    print(f\"\\nüéâ 32-class confidence-calibrated training completed successfully!\")\n",
    "    print(f\"üîß Fixed: Used direct data approach to avoid TensorFlow 2.14+ generator issues\")\n",
    "    \n",
    "    # Global variables for next cells\n",
    "    globals()['training_completed'] = True\n",
    "    globals()['model_trained'] = True\n",
    "    \n",
    "    # Clean up memory\n",
    "    del augmented_x, augmented_y\n",
    "    import gc\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e3188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive evaluation of 32-class model performance and calibration\n",
    "\n",
    "print(\"üìä COMPREHENSIVE 32-CLASS MODEL EVALUATION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Load best model weights\n",
    "print(\"üîÑ Loading best model weights...\")\n",
    "best_model_path = f'{MODEL_SAVE_PATH}/QuickDraw_CALIBRATED_32class_{TARGET_SIZE}x{TARGET_SIZE}.keras'\n",
    "model.load_weights(best_model_path)\n",
    "print(\"‚úÖ Best weights loaded!\")\n",
    "\n",
    "# Standard evaluation metrics\n",
    "print(f\"\\nüìà Standard Evaluation Metrics:\")\n",
    "test_loss, test_acc, test_top5_acc = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(f\"   Test Accuracy: {test_acc:.4f} ({test_acc*100:.1f}%)\")\n",
    "print(f\"   Test Top-5 Accuracy: {test_top5_acc:.4f} ({test_top5_acc*100:.1f}%)\")\n",
    "print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Detailed calibration analysis\n",
    "print(f\"\\nüî¨ Generating predictions for calibration analysis...\")\n",
    "test_predictions = model.predict(test_x, verbose=1)\n",
    "\n",
    "# Confidence statistics\n",
    "max_confidences = np.max(test_predictions, axis=1)\n",
    "predicted_classes = np.argmax(test_predictions, axis=1)\n",
    "true_classes = np.argmax(test_y, axis=1)\n",
    "\n",
    "# Overall confidence statistics\n",
    "avg_confidence = np.mean(max_confidences)\n",
    "median_confidence = np.median(max_confidences)\n",
    "std_confidence = np.std(max_confidences)\n",
    "\n",
    "print(f\"\\nüéØ Confidence Calibration Analysis:\")\n",
    "print(f\"   Average confidence: {avg_confidence:.3f} ({avg_confidence*100:.1f}%)\")\n",
    "print(f\"   Median confidence: {median_confidence:.3f} ({median_confidence*100:.1f}%)\")\n",
    "print(f\"   Standard deviation: {std_confidence:.3f}\")\n",
    "\n",
    "# Confidence distribution analysis\n",
    "confidence_ranges = [\n",
    "    (0.0, 0.3, \"Low\"),\n",
    "    (0.3, 0.5, \"Moderate-Low\"), \n",
    "    (0.5, 0.7, \"Moderate\"),\n",
    "    (0.7, 0.8, \"Moderate-High\"),\n",
    "    (0.8, 0.9, \"High\"),\n",
    "    (0.9, 0.95, \"Very High\"),\n",
    "    (0.95, 1.0, \"Extremely High\")\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä Confidence Distribution:\")\n",
    "for low, high, label in confidence_ranges:\n",
    "    count = np.sum((max_confidences >= low) & (max_confidences < high))\n",
    "    percentage = count / len(max_confidences) * 100\n",
    "    print(f\"   {label:15} ({low:.1f}-{high:.1f}): {count:5d} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "# Per-class accuracy and confidence analysis\n",
    "print(f\"\\nüé® Per-Class Analysis (Top 10 classes by accuracy):\")\n",
    "class_accuracies = []\n",
    "class_confidences = []\n",
    "\n",
    "for class_id in range(NUM_CLASSES):\n",
    "    class_mask = true_classes == class_id\n",
    "    if np.sum(class_mask) > 0:\n",
    "        class_pred = predicted_classes[class_mask]\n",
    "        class_conf = max_confidences[class_mask]\n",
    "        \n",
    "        class_acc = np.mean(class_pred == class_id)\n",
    "        class_avg_conf = np.mean(class_conf)\n",
    "        \n",
    "        class_accuracies.append((class_id, class_acc, class_avg_conf, np.sum(class_mask)))\n",
    "        class_confidences.append(class_avg_conf)\n",
    "\n",
    "# Sort by accuracy and display top 10\n",
    "class_accuracies.sort(key=lambda x: x[1], reverse=True)\n",
    "for i, (class_id, acc, conf, samples) in enumerate(class_accuracies[:10]):\n",
    "    class_name = QUICKDRAW_32_CLASSES[class_id] if class_id < len(QUICKDRAW_32_CLASSES) else f\"Class_{class_id}\"\n",
    "    print(f\"   {i+1:2d}. {class_name:15} - Acc: {acc:.3f} | Conf: {conf:.3f} | Samples: {samples:4d}\")\n",
    "\n",
    "# Calibration metrics\n",
    "calibration_error = abs(avg_confidence - test_acc)\n",
    "print(f\"\\nüìè Calibration Metrics:\")\n",
    "print(f\"   Expected Calibration Error: {calibration_error:.3f}\")\n",
    "\n",
    "if calibration_error < 0.05:\n",
    "    calibration_quality = \"Excellent\"\n",
    "    calibration_emoji = \"üéâ\"\n",
    "elif calibration_error < 0.1:\n",
    "    calibration_quality = \"Good\" \n",
    "    calibration_emoji = \"‚úÖ\"\n",
    "elif calibration_error < 0.15:\n",
    "    calibration_quality = \"Fair\"\n",
    "    calibration_emoji = \"‚ö†Ô∏è\"\n",
    "else:\n",
    "    calibration_quality = \"Poor\"\n",
    "    calibration_emoji = \"‚ùå\"\n",
    "\n",
    "print(f\"   Calibration Quality: {calibration_emoji} {calibration_quality}\")\n",
    "\n",
    "# Overconfidence analysis\n",
    "overconfident_90 = np.sum(max_confidences > 0.9) / len(max_confidences) * 100\n",
    "overconfident_95 = np.sum(max_confidences > 0.95) / len(max_confidences) * 100\n",
    "well_calibrated = np.sum((max_confidences >= 0.4) & (max_confidences <= 0.8)) / len(max_confidences) * 100\n",
    "\n",
    "print(f\"\\nüö® Confidence Quality Assessment:\")\n",
    "print(f\"   Overconfident (>90%): {overconfident_90:.1f}%\")\n",
    "print(f\"   Extremely overconfident (>95%): {overconfident_95:.1f}%\")\n",
    "print(f\"   Well-calibrated (40-80%): {well_calibrated:.1f}%\")\n",
    "\n",
    "# Overall assessment\n",
    "if avg_confidence < 0.75 and calibration_error < 0.1:\n",
    "    assessment = \"üéâ EXCELLENT: Well-calibrated confidence achieved for 32 classes!\"\n",
    "    recommendation = \"‚úÖ Ready for production deployment\"\n",
    "elif avg_confidence < 0.85 and calibration_error < 0.15:\n",
    "    assessment = \"‚úÖ GOOD: Much better calibration than typical models\"\n",
    "    recommendation = \"üëç Suitable for QuickDraw game with minor threshold tuning\"\n",
    "else:\n",
    "    assessment = \"‚ö†Ô∏è MODERATE: Some overconfidence detected\"\n",
    "    recommendation = \"üîß Consider additional calibration techniques\"\n",
    "\n",
    "print(f\"\\nüéØ Overall Assessment:\")\n",
    "print(f\"   {assessment}\")\n",
    "print(f\"   {recommendation}\")\n",
    "\n",
    "# Confusion matrix visualization for top classes\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Select top 16 classes for visualization\n",
    "top_16_classes = [x[0] for x in class_accuracies[:16]]\n",
    "mask_top16 = np.isin(true_classes, top_16_classes) & np.isin(predicted_classes, top_16_classes)\n",
    "\n",
    "if np.sum(mask_top16) > 0:\n",
    "    true_top16 = true_classes[mask_top16]\n",
    "    pred_top16 = predicted_classes[mask_top16]\n",
    "    \n",
    "    # Map to 0-15 range\n",
    "    class_mapping = {old_id: new_id for new_id, old_id in enumerate(top_16_classes)}\n",
    "    true_mapped = np.array([class_mapping[x] for x in true_top16])\n",
    "    pred_mapped = np.array([class_mapping[x] for x in pred_top16])\n",
    "    \n",
    "    cm = confusion_matrix(true_mapped, pred_mapped)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[QUICKDRAW_32_CLASSES[i][:8] if i < len(QUICKDRAW_32_CLASSES) else f'C{i}' for i in top_16_classes],\n",
    "                yticklabels=[QUICKDRAW_32_CLASSES[i][:8] if i < len(QUICKDRAW_32_CLASSES) else f'C{i}' for i in top_16_classes])\n",
    "    plt.title('Confusion Matrix - Top 16 Classes (32-Class Model)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('confusion_matrix_32classes.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Comprehensive evaluation completed!\")\n",
    "print(f\"üìä Model ready for confidence-aware deployment in QuickDraw game!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c53246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and download the trained 32-class model\n",
    "\n",
    "print(\"üíæ SAVING AND PREPARING MODEL FOR DOWNLOAD\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Save model in multiple formats for maximum compatibility\n",
    "model_base_name = f\"QuickDraw_CALIBRATED_32class_{TARGET_SIZE}x{TARGET_SIZE}\"\n",
    "\n",
    "# 1. Save as Keras format (recommended)\n",
    "keras_path = f\"{MODEL_SAVE_PATH}/{model_base_name}.keras\"\n",
    "model.save(keras_path)\n",
    "print(f\"‚úÖ Keras model saved: {keras_path}\")\n",
    "\n",
    "# 2. Save as H5 format (legacy compatibility)\n",
    "h5_path = f\"{MODEL_SAVE_PATH}/{model_base_name}.h5\"\n",
    "model.save(h5_path)\n",
    "print(f\"‚úÖ H5 model saved: {h5_path}\")\n",
    "\n",
    "# 3. Save weights only (for custom model reconstruction)\n",
    "weights_path = f\"{MODEL_SAVE_PATH}/{model_base_name}_weights.h5\"\n",
    "model.save_weights(weights_path)\n",
    "print(f\"‚úÖ Model weights saved: {weights_path}\")\n",
    "\n",
    "# 4. Convert to TensorFlow Lite for mobile deployment\n",
    "print(f\"\\nüîÑ Converting to TensorFlow Lite...\")\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = f\"{MODEL_SAVE_PATH}/{model_base_name}.tflite\"\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"‚úÖ TensorFlow Lite model saved: {tflite_path}\")\n",
    "\n",
    "# 5. Save model configuration and metadata\n",
    "model_config = {\n",
    "    'model_name': model_base_name,\n",
    "    'num_classes': NUM_CLASSES,\n",
    "    'input_shape': [TARGET_SIZE, TARGET_SIZE, 1],\n",
    "    'architecture': 'Confidence-Calibrated CNN',\n",
    "    'training_config': {\n",
    "        'epochs': EPOCHS,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'target_size': TARGET_SIZE,\n",
    "        'use_mixup': USE_MIXUP,\n",
    "        'label_smoothing': 0.1,\n",
    "        'temperature_scaling': True\n",
    "    },\n",
    "    'performance_metrics': {\n",
    "        'test_accuracy': float(test_acc),\n",
    "        'test_top5_accuracy': float(test_top5_acc),\n",
    "        'test_loss': float(test_loss),\n",
    "        'avg_confidence': float(avg_confidence),\n",
    "        'calibration_error': float(calibration_error)\n",
    "    },\n",
    "    'class_names': QUICKDRAW_32_CLASSES[:NUM_CLASSES],\n",
    "    'calibration_info': {\n",
    "        'avg_confidence': float(avg_confidence),\n",
    "        'median_confidence': float(median_confidence),\n",
    "        'std_confidence': float(std_confidence),\n",
    "        'well_calibrated_percentage': float(well_calibrated),\n",
    "        'overconfident_90_percentage': float(overconfident_90),\n",
    "        'overconfident_95_percentage': float(overconfident_95)\n",
    "    },\n",
    "    'usage_instructions': {\n",
    "        'preprocessing': f\"Resize to {TARGET_SIZE}x{TARGET_SIZE}, normalize to [0,1]\",\n",
    "        'confidence_threshold': \"Recommended: 0.6-0.7 for good balance\",\n",
    "        'temperature_scaling': \"Built-in, no post-processing needed\"\n",
    "    }\n",
    "}\n",
    "\n",
    "config_path = f\"{MODEL_SAVE_PATH}/{model_base_name}_config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(model_config, f, indent=2)\n",
    "print(f\"‚úÖ Model configuration saved: {config_path}\")\n",
    "\n",
    "# 6. Create a complete deployment package\n",
    "print(f\"\\nüì¶ Creating deployment package...\")\n",
    "package_name = f\"{model_base_name}_complete_package.zip\"\n",
    "package_path = f\"{MODEL_SAVE_PATH}/{package_name}\"\n",
    "\n",
    "with zipfile.ZipFile(package_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Add model files\n",
    "    zipf.write(keras_path, os.path.basename(keras_path))\n",
    "    zipf.write(h5_path, os.path.basename(h5_path))\n",
    "    zipf.write(weights_path, os.path.basename(weights_path))\n",
    "    zipf.write(tflite_path, os.path.basename(tflite_path))\n",
    "    zipf.write(config_path, os.path.basename(config_path))\n",
    "    \n",
    "    # Add training plots if they exist\n",
    "    for plot_file in ['training_history_32classes.png', 'confusion_matrix_32classes.png', 'class_distribution_32classes.png']:\n",
    "        if os.path.exists(plot_file):\n",
    "            zipf.write(plot_file, plot_file)\n",
    "    \n",
    "    # Create README for the package\n",
    "    readme_content = f\"\"\"# QuickDraw 32-Class Confidence-Calibrated Model\n",
    "\n",
    "## Model Information\n",
    "- **Model Name**: {model_base_name}\n",
    "- **Classes**: {NUM_CLASSES}\n",
    "- **Input Size**: {TARGET_SIZE}x{TARGET_SIZE}x1\n",
    "- **Test Accuracy**: {test_acc*100:.1f}%\n",
    "- **Top-5 Accuracy**: {test_top5_acc*100:.1f}%\n",
    "- **Average Confidence**: {avg_confidence*100:.1f}%\n",
    "- **Calibration Error**: {calibration_error:.3f}\n",
    "\n",
    "## Files Included\n",
    "1. `{model_base_name}.keras` - Full Keras model (recommended)\n",
    "2. `{model_base_name}.h5` - H5 format for compatibility\n",
    "3. `{model_base_name}_weights.h5` - Model weights only\n",
    "4. `{model_base_name}.tflite` - TensorFlow Lite for mobile\n",
    "5. `{model_base_name}_config.json` - Complete model configuration\n",
    "6. Training visualizations (PNG files)\n",
    "\n",
    "## Usage Instructions\n",
    "\n",
    "### Python/Keras Loading:\n",
    "```python\n",
    "import tensorflow as tf\n",
    "model = tf.keras.models.load_model('{model_base_name}.keras')\n",
    "```\n",
    "\n",
    "### Preprocessing:\n",
    "1. Resize images to {TARGET_SIZE}x{TARGET_SIZE}\n",
    "2. Convert to grayscale\n",
    "3. Normalize pixel values to [0, 1] range\n",
    "4. Reshape to (batch_size, {TARGET_SIZE}, {TARGET_SIZE}, 1)\n",
    "\n",
    "### Confidence Interpretation:\n",
    "- **0.0-0.4**: Low confidence (unreliable prediction)\n",
    "- **0.4-0.7**: Moderate confidence (good for most uses)\n",
    "- **0.7-0.9**: High confidence (very reliable)\n",
    "- **0.9+**: Very high confidence (rare due to calibration)\n",
    "\n",
    "### Recommended Confidence Threshold: 0.6-0.7\n",
    "\n",
    "## Classes:\n",
    "{chr(10).join([f\"{i}: {name}\" for i, name in enumerate(QUICKDRAW_32_CLASSES[:NUM_CLASSES])])}\n",
    "\n",
    "## Calibration Features:\n",
    "- ‚úÖ Label smoothing (0.1)\n",
    "- ‚úÖ Temperature scaling (learnable)\n",
    "- ‚úÖ Monte Carlo dropout\n",
    "- ‚úÖ Enhanced regularization\n",
    "- ‚úÖ 32-class optimized architecture\n",
    "\n",
    "Generated on: {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "    \n",
    "    zipf.writestr(\"README.md\", readme_content)\n",
    "\n",
    "print(f\"‚úÖ Complete package created: {package_path}\")\n",
    "\n",
    "# Get file sizes\n",
    "file_sizes = {}\n",
    "for file_path, description in [\n",
    "    (keras_path, \"Keras model\"),\n",
    "    (h5_path, \"H5 model\"), \n",
    "    (tflite_path, \"TensorFlow Lite\"),\n",
    "    (package_path, \"Complete package\")\n",
    "]:\n",
    "    size_mb = os.path.getsize(file_path) / 1024**2\n",
    "    file_sizes[description] = size_mb\n",
    "\n",
    "print(f\"\\nüìä File Sizes:\")\n",
    "for desc, size in file_sizes.items():\n",
    "    print(f\"   {desc}: {size:.1f} MB\")\n",
    "\n",
    "# Download files from Colab\n",
    "print(f\"\\n‚¨áÔ∏è Downloading files...\")\n",
    "try:\n",
    "    # Download the complete package\n",
    "    files.download(package_path)\n",
    "    print(f\"‚úÖ Downloaded: {package_name}\")\n",
    "    \n",
    "    # Optionally download individual files\n",
    "    download_individual = input(\"Download individual model files too? (y/n): \").lower().strip()\n",
    "    if download_individual == 'y':\n",
    "        files.download(keras_path)\n",
    "        files.download(tflite_path)\n",
    "        files.download(config_path)\n",
    "        print(\"‚úÖ Individual files downloaded!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Download error: {e}\")\n",
    "    print(\"üí° You can manually download files from the file browser\")\n",
    "\n",
    "# Final summary and instructions\n",
    "print(f\"\\nüéâ MODEL DEPLOYMENT READY!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìã Deployment Summary:\")\n",
    "print(f\"   ‚úÖ Model trained on {NUM_CLASSES} classes\")\n",
    "print(f\"   ‚úÖ Confidence calibration applied\")\n",
    "print(f\"   ‚úÖ Test accuracy: {test_acc*100:.1f}%\")\n",
    "print(f\"   ‚úÖ Average confidence: {avg_confidence*100:.1f}%\")\n",
    "print(f\"   ‚úÖ Calibration error: {calibration_error:.3f}\")\n",
    "print(f\"   ‚úÖ Multiple format exports completed\")\n",
    "\n",
    "print(f\"\\nüîÑ Next Steps for QuickDraw Game Integration:\")\n",
    "print(f\"   1. Download the complete package: {package_name}\")\n",
    "print(f\"   2. Extract and use {model_base_name}.keras in your backend\")\n",
    "print(f\"   3. Update your drawing_model.py to load this model\")\n",
    "print(f\"   4. Set confidence threshold to 0.6-0.7 for optimal performance\")\n",
    "print(f\"   5. Test with real drawings from your QuickDraw game\")\n",
    "print(f\"   6. Fine-tune confidence threshold based on user experience\")\n",
    "\n",
    "print(f\"\\nüí° Model Features:\")\n",
    "print(f\"   üéØ Realistic confidence scores (not overconfident)\")\n",
    "print(f\"   üöÄ Optimized for 32-class classification\")\n",
    "print(f\"   üì± TensorFlow Lite ready for mobile deployment\")\n",
    "print(f\"   üîß Built-in calibration (no post-processing needed)\")\n",
    "print(f\"   üìä Comprehensive evaluation metrics included\")\n",
    "\n",
    "print(f\"\\nüéÆ Ready to enhance your QuickDraw game with realistic AI confidence!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
